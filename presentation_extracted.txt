NLP
Project
2
T
as
ks
1{4
F
eb
rua
ry
26,
2026
r
y
1
/
13
What
W
e
Built
A
full
pip
eline
across
four
tasks:
ask
1:
unigram/bigram/trigram
language
mo
delin
g
(MLE)
2:
smo
othing
c
ompa
rison
fo
n
-
gr
am
LMs
3:
sentiment
classi cation
(NB
and
Logistic
Regression)
4:
dot-as-sentence
-b
ounda
bina
Data
Split
Setup
rain
Dev
est
Corpora/News/corp
us.txt
96,000
12,000
(80/10/10)
3
dataset/data
set
v1.csv
25,600
6,400
8,000
(70/15/15)
4
dot
labeled
data.csv
140,000
30,000
use
strati ed
splits.
k
N-gram
LM
with
MLE
Mo
deli
ng
decisions
ok
enized
news
sentences
trained
unigram,
bigram,
trigram
d
ls.
Replaced
ra
re
training
tok
ens
(
<
frequency)
<UNK>
.
Evaluated
b
p
erplexit
on
held-out
test
sentences.
Key
result
Metric
V
alue
Unigram
PPL
3296.2567
Bigram
rigram
ak
ea
w
a
y:
plain
fails
higher-o
rder
n-grams
ecause
unseen
events
ge
t
zero
robabilit
Smo
Co
mp
compa
red
Laplace,
interp
olation,
back
o ,
Kneser{Ney
f
o
bigram
igram
dels.
Interp
olation
eights
tuned
dev
set:
Bigram:

;
)
=
(0
:
0
8)
rigra
m
5)
Metho
Laplace
3572.4528
12556.2180
167.9878
88.0896
Back
158.2786
74.7584
150.0579
70.7616
5
retation
oth
+
outp
erfo
rmed
othing,
so
extra
context
useful
after
spa
rsit
xed.
gave
the
rst
la
rge
rgin
in
this
setup.
6
Sentiment
Classi
cation
dels
Multinomial
Naive
Ba
es
Bernoulli
Regre
ssion
liblinear
eature
sets
bow
CountV
ecto
rizer
unigr
min
df=2
,
max
features=30000
lexicon
handcrafted
sentiment/negation
features
concate
ation
of
uning
NB:
05
LR:
C
10
class
none
balanced
Selection
me
tr
ic
macro-F1
7
Main
Results
del
Best
setting
c.
MNB
0.8491
0.8490
BNB
0.8528
0.8547
0.8550
LR
3,
0.8974
0.9009
0.9010
classi e
r:
Regression
McNema
tests
sho
is
signi c
ntly
stronger
than
NB
va
riants

0).
8
Help
ed
Wha
Did
Not
Lexicon-only
ere
eak
64
macro-F1),
they
cannot
replace
text
n-grams.
But
adding
l
xicon
cues
top
BoW
small
but
real
lift:
8992
!
9009
(test
macro-F1)
This
suggests
ola
rit
negation
still
add
signal
when
ombin
data-driven
features.
9
Dot
Bounda
Classi er
Bina
ass
i cation
Inputs:
all
columns
except
label
vecto
rized
ith
DictVectorizer
del:
L2
L1
regula
ization.
uned
01
100
de
v
F1.
decision
threshold
dev,
scanning
0.10
to
0.89
(step
0.01).
vs
P
enalt
hr.
F1
Acc.
Prec.
Rec.
0.64
0.9993
0.9992
0.9980
0.9990
0.9985
0.59
0.9988
Final
selection:
(highest
F1).
11
UI
Demo
La
er
Added
interfaces:
results
ui.py
Tkinter
dashb
oa
rd
run
insp
ect
metrics
quickly
localhost
Streamlit
lo
cal
app
plus
interactive
demos.
side
includes
ractical
mos
sentence
analysis
rison,
custom-text
rediction,
dot-b
redictions
user
text.
12
ys
alone
not
enough
n-gra
LMs;
essential.
most
reliable
metho
dataset.
sentiment,
combined
BoW/lexicon
st.
detection,
very
strong,
sligh
tly
etter.
The
ols
made
it
s
i
verify
nd
withou
changing
co