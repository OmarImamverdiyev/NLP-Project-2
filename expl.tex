\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}

\title{Detailed Explanation of \texttt{presentation.tex}}
\author{}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Goal of This Document}

This file explains the logic behind each slide in \texttt{presentation.tex}:
\begin{itemize}
  \item what each slide is trying to communicate,
  \item why each modeling choice was made,
  \item what the reported numbers mean,
  \item and how to defend the conclusions in discussion or Q\&A.
\end{itemize}

\section{Overall Story of the Presentation}

The presentation is organized as a pipeline:
\begin{itemize}
  \item Task 1 establishes a baseline language modeling setup using pure MLE.
  \item Task 2 fixes Task 1's sparsity problem using smoothing and compares methods.
  \item Task 3 moves to supervised sentiment classification and compares standard classifiers.
  \item Task 4 solves a very high-accuracy boundary detection problem with logistic regression.
  \item The UI slide shows practical usability: not only results, but runnable demos.
\end{itemize}

So the narrative is not a list of unrelated tasks. It is: ``build baselines, identify limitations, improve methods, evaluate rigorously, and package for practical use.''

\section{Slide-by-Slide Explanation}

\subsection{Title Page}

\textbf{Why it is there:} It defines scope quickly: one project, four tasks, one integrated workflow.

\textbf{What to emphasize verbally:} ``This is a complete mini NLP system, not a single model experiment.''

\subsection{What We Built}

\textbf{Why this slide exists:}
It gives the audience a map before details. Each task corresponds to a common NLP problem class:
\begin{itemize}
  \item language modeling (generative probability modeling),
  \item smoothing and generalization under sparsity,
  \item sentiment classification (supervised text classification),
  \item sentence boundary detection (token-level binary classification).
\end{itemize}

\textbf{Why this framing matters:}
It signals breadth and technical progression from classical count models to discriminative classifiers.

\subsection{Data and Split Setup}

\textbf{Why the data table is critical:}
Performance numbers are meaningless without dataset and split details. This slide supports reproducibility and fairness.

\textbf{How to read the split counts:}
\begin{itemize}
  \item Task 1 and Task 2 use \texttt{Corpora/News/corpus.txt} with $700{,}000$ total sentences.
  \item 80/10/10 gives:
  \begin{itemize}
    \item train: $560{,}000$,
    \item dev: $70{,}000$,
    \item test: $70{,}000$.
  \end{itemize}
  \item Task 3 and Task 4 use separate labeled datasets and 70/15/15 splits.
\end{itemize}

\textbf{Why stratified splits are explicitly mentioned for Task 3 and Task 4:}
Class imbalance can distort metrics. Stratification preserves label proportions across train/dev/test, making comparisons more reliable.

\subsection{Task 1: N-gram LM with MLE}

\subsubsection*{Why these modeling decisions}
\begin{itemize}
  \item \textbf{Tokenization:} n-gram models operate on token sequences, so tokenization is mandatory.
  \item \textbf{Unigram, bigram, trigram:} this shows increasing context size and sparsity effects.
  \item \textbf{Rare-word replacement with \texttt{<UNK>}:} reduces vocabulary explosion and improves robustness.
  \item \textbf{Perplexity on held-out test:} standard intrinsic metric for language models.
\end{itemize}

\subsubsection*{Why perplexity is the right metric here}
Perplexity is essentially exponentiated average negative log-likelihood:
\[
\text{PPL} = \exp\!\left(-\frac{1}{N}\sum_{i=1}^{N}\log p(w_i \mid h_i)\right).
\]
Lower PPL means better predictive probability on unseen text.

\subsubsection*{How to interpret the results}
\begin{itemize}
  \item Unigram MLE PPL: $3759.5357$.
  \item Bigram MLE PPL: $\infty$.
  \item Trigram MLE PPL: $\infty$.
\end{itemize}

\textbf{Why bigram/trigram become infinity under MLE:}
MLE assigns zero probability to unseen n-grams. If even one test event has probability 0, log-likelihood contains $-\infty$, so perplexity becomes $\infty$.

\textbf{Main conceptual takeaway:}
Higher-order context helps only if you can handle sparsity. Raw MLE cannot.

\subsection{Task 2: Smoothing Comparison}

\subsubsection*{Why this task follows Task 1}
Task 1 exposes a failure mode. Task 2 is the direct fix: keep n-gram context, remove zero-probability failures with smoothing.

\subsubsection*{Why these smoothing methods were selected}
\begin{itemize}
  \item \textbf{Laplace:} simple additive baseline; often too blunt but useful reference.
  \item \textbf{Interpolation:} combines lower and higher-order models smoothly.
  \item \textbf{Backoff:} uses higher-order counts when available, backs off otherwise.
  \item \textbf{Kneser--Ney:} strong classic method, especially good for continuation behavior.
\end{itemize}

\subsubsection*{Why interpolation lambdas are tuned on dev}
Weights control bias-variance tradeoff. Dev tuning prevents overfitting to test while adapting to this corpus.

Reported tuned weights:
\begin{itemize}
  \item Bigram interpolation: $(\lambda_1,\lambda_2)=(0.1,0.9)$.
  \item Trigram interpolation: $(\lambda_1,\lambda_2,\lambda_3)=(0.1,0.3,0.6)$.
\end{itemize}

\textbf{Interpretation of these lambdas:}
The model relies mostly on higher-order context but still keeps nonzero lower-order support.

\subsubsection*{How to interpret the perplexity table}
\begin{center}
\begin{tabular}{lrr}
\toprule
Method & Bigram PPL & Trigram PPL \\
\midrule
Laplace & 2958.8623 & 13576.9329 \\
Interpolation & 156.8298 & 70.8347 \\
Backoff & 148.2803 & 61.2244 \\
Kneser--Ney & 141.2498 & 58.2074 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Why these numbers make sense:}
\begin{itemize}
  \item Laplace is worst because it over-distributes probability mass to many unlikely events.
  \item Interpolation and backoff are much better because they control sparsity more intelligently.
  \item Kneser--Ney is best, consistent with known behavior on sparse n-gram language modeling.
  \item Trigram $<$ bigram for strong methods, so extra context helps once smoothing is done well.
\end{itemize}

\subsection{Task 2 Interpretation Slide}

\textbf{Why this separate interpretation slide is good:}
The previous slide is numeric; this slide converts numbers into claims:
\begin{itemize}
  \item best method: Kneser--Ney,
  \item context helps after smoothing,
  \item Laplace baseline is poor in this setup.
\end{itemize}

\textbf{What this proves in project logic:}
Task 2 resolves Task 1's failure and gives a justified modeling recommendation for this dataset.

\subsection{Task 3: Sentiment Classification Setup}

\subsubsection*{Why these models}
\begin{itemize}
  \item \textbf{MNB and BNB:} strong classical baselines, fast and interpretable.
  \item \textbf{Logistic Regression:} discriminative linear model that usually outperforms NB with strong features.
\end{itemize}

\subsubsection*{Why these feature sets}
\begin{itemize}
  \item \texttt{bow}: high-coverage lexical signal from unigrams+bigrams.
  \item \texttt{lexicon}: compact handcrafted sentiment/negation signal.
  \item \texttt{bow\_lexicon}: tests complementarity between data-driven and handcrafted features.
\end{itemize}

\subsubsection*{Why tuning choices are reasonable}
\begin{itemize}
  \item $\alpha$ search for NB controls smoothing strength.
  \item $C$ and class weighting for LR control regularization and imbalance handling.
  \item Dev macro-F1 as selection metric is robust when class distribution may be uneven.
\end{itemize}

\subsection{Task 3: Main Results}

\textbf{Reported best results:}
\begin{itemize}
  \item MNB best test macro-F1: $0.8490$.
  \item BNB best test macro-F1: $0.8547$.
  \item LR best test macro-F1: $0.9009$, test accuracy: $0.9010$.
\end{itemize}

\textbf{Why LR wins here:}
With sparse text features, linear discriminative training usually learns decision boundaries better than NB's conditional independence assumptions.

\textbf{Why McNemar is included:}
It tests paired prediction disagreement significance, not just average score differences. This makes the ``LR is better'' claim statistically defensible.

\subsection{Task 3: What Helped and What Did Not}

\textbf{Why this slide is strong:}
It separates useful vs non-useful additions:
\begin{itemize}
  \item lexicon-only is weak ($\approx 0.64$ macro-F1),
  \item but lexicon + BoW gives a measurable lift for LR ($0.8992 \to 0.9009$).
\end{itemize}

\textbf{Interpretation:}
Handcrafted features are not enough by themselves but can still add marginal value on top of rich lexical features.

\subsection{Task 4: Dot Boundary Classifier Setup}

\textbf{Why this is framed as binary classification:}
Each dot candidate is a yes/no decision: sentence boundary or not.

\textbf{Why DictVectorizer is used:}
Feature columns are dictionary-like structured attributes, so sparse one-hot style vectorization is appropriate.

\textbf{Why compare L1 and L2 LR:}
\begin{itemize}
  \item L2 keeps many small weights (dense-ish solution).
  \item L1 promotes sparse weights (feature selection effect).
\end{itemize}

\textbf{Why tune both $C$ and threshold:}
Optimizing only model weights can be suboptimal for F1. Threshold tuning aligns decision boundary with the desired precision/recall tradeoff.

\subsection{Task 4: L1 vs L2 Results}

\textbf{Reported numbers show near-ceiling performance:}
\begin{itemize}
  \item L2 test F1: $0.9985$.
  \item L1 test F1: $0.9988$ (selected).
\end{itemize}

\textbf{Why selecting L1 is reasonable:}
The gain is small but consistent with best test F1 and slightly better precision at similar recall.

\textbf{Caution to mention in oral defense:}
When scores are this high, data leakage checks and split discipline matter a lot. The project indicates fixed train/dev/test usage, which is the right protocol.

\subsection{UI and Demo Layer}

\textbf{Why this slide matters technically:}
It proves the project is operational:
\begin{itemize}
  \item reproducible task runs,
  \item quick metric inspection,
  \item interactive demonstrations for all major task types.
\end{itemize}

\textbf{Why this improves project quality:}
It reduces friction for graders and collaborators and supports rapid qualitative sanity checks.

\subsection{Final Takeaways}

\textbf{Why each bullet is justified by previous slides:}
\begin{itemize}
  \item ``MLE is not enough'' is proven by $\infty$ bigram/trigram PPL in Task 1.
  \item ``Kneser--Ney is best'' is supported by lowest Task 2 PPL.
  \item ``LR + combined features best for sentiment'' follows Task 3 table.
  \item ``L1 slightly better for dot boundary'' follows Task 4 table.
  \item ``UI helps validation'' follows the system/demo slide.
\end{itemize}

\section{How to Present This Clearly in 2--3 Minutes per Task}

\subsection{Task 1 speaking template}
\begin{itemize}
  \item Start with goal: build baseline n-gram LMs.
  \item State issue: higher-order MLE gives zero probabilities, so PPL becomes infinity.
  \item End with transition: this motivates smoothing in Task 2.
\end{itemize}

\subsection{Task 2 speaking template}
\begin{itemize}
  \item Explain smoothing purpose: avoid zero-probability failures.
  \item Point to ranking: Kneser--Ney $<$ backoff $<$ interpolation $\ll$ Laplace.
  \item Conclude: trigram context helps when smoothing is done right.
\end{itemize}

\subsection{Task 3 speaking template}
\begin{itemize}
  \item Explain model and feature comparison setup.
  \item Report winner: LR + BoW+lexicon.
  \item Add nuance: lexicon-only weak, but useful as additive signal.
\end{itemize}

\subsection{Task 4 speaking template}
\begin{itemize}
  \item Frame as high-precision binary classification.
  \item Explain L1 vs L2 and threshold tuning.
  \item Conclude: both excellent, L1 best by test F1.
\end{itemize}

\section{Common Questions and Good Answers}

\textbf{Q: Why use perplexity for Task 1/2?}\\
A: The tasks are language modeling tasks, and perplexity is the standard intrinsic metric for next-token probability quality.

\textbf{Q: Why does MLE fail for bigram/trigram but not unigram?}\\
A: With larger context, unseen combinations are common. Any unseen context-token event gets probability 0 under MLE.

\textbf{Q: Why trust Kneser--Ney?}\\
A: It is both theoretically strong for continuation modeling and empirically best in your table.

\textbf{Q: Why macro-F1 for Task 3 selection?}\\
A: Macro-F1 balances per-class performance and is less biased by class frequency than plain accuracy.

\textbf{Q: Why tune threshold in Task 4?}\\
A: Probability calibration is not perfect by default; threshold tuning directly optimizes target behavior (here F1).

\section{Bottom Line}

This presentation is coherent because each slide either:
\begin{itemize}
  \item defines setup needed for valid evaluation,
  \item reports empirical evidence,
  \item or turns evidence into justified conclusions.
\end{itemize}

If you present it as ``problem $\rightarrow$ method $\rightarrow$ metric $\rightarrow$ evidence $\rightarrow$ conclusion,'' every major claim is defendable.

\end{document}
