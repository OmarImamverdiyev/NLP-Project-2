NLP
Pro
ject
2
Rep
ort
(T
asks
1{4)
F
ebruary
26,
2026
1
Exp
erimen
tal
Setup
I
prepared
this
rep
from
the
sa
v
ed
run
outputs
in
test-results.txt
and
task
scripts
Task1--Task4
(no
reru
ns).
T
able
summarizes
ho
w
eac
h
as
split
to
train
/dev/te
st.
1:
rain/dev/test
c
haracteristics
ask
Data
source
Split
strategy
Actual
coun
ts
corpus.txt
Random
sen
tence
split:
80%
/
10%
dev
test
(seed
42)
96,000
12,
000
12,000
(from
120,000)
3
dataset
v1.csv
Strati ed
p
o
ol
+
20%
test,
then
of
for
25,600
6,400
8,000
40,000)
4
dot
labeled
data.csv
70%
30%
temp,
temp
50/50
dev/test
140,000
30,000
200,000)
N-gram
Language
Mo
deling
(M
LE)
deling.
trained
u
nigram,
bigram,
trigram
language
mo
dels
with
plain
M
LE.
ok
enization
splitting
came
shared
utilities;
rare
training
ords
(frequency
<
2)
ere
mapp
<UNK>
,
same
cabulary
mapping
applied
dev/test.
Result.
2:
results
(MLE
erpl
e
xi
t
y)
Metric
V
alue
Num
b
er
tences
120,000
size
(after
handlin
g)
58,418
Unigram
MLE
erplexit
y
3296.2567
Bigram
rigram
The
eha
vior
is
exp
ec
d
:
without
smo
othing,
one
unseen
bigram/trigram
mak
es
probabi
lit
zero,
so
ecomes
in nite.
Smo
othing
f
or
B
igram/T
LMs
reused
th
LM
pip
eline
compared
four
metho
ds:
Laplace,
linear
terp
olation,
absolute-discoun
bac
k
o ,
Kneser{Ney
.
In
olation
i
gh
tuned
on
using
grid
searc
step
0.1.
Best
lam
(

;
)
=
(0
0
8)
bigram
5)
ation.
3:
Metho
PPL
Laplace
3572.4528
12556.2180
167.
9878
88.0896
Bac
(discoun
0.75)
158.2786
74.7584
150.0579
70.7616
est
oth
trigram.
Also,
clearly
out-
erformed
whic
means
extra
con
text
help
once
sparsit
handled
correctly
Sen
timen
Classi cation
three
classi ers:
ultinomial
NB,
Bernoulli
Logistic
Regression.
represen
tati
on,
used
feature
sets:
bow
lexicon
part
s
CountVectorizer
unigram+bigram
features
ngram
range=(1,2)
),
min
df=2
max
features=30000
blo
has
6
handcrafted
time
n
cues
(p
osi-
tiv
e/negativ
olarit
di erence,
normalized
exclamation
ag,
negation-a
are
signal).
tu
ned
NB
05
5
g
LR
C
10
class
weight
None
balanced
selecting
macro-F1.
Result
(b
set
del).
4:
del
comparison
Be
st
yp
erparame-
ter(s)
Dev
macro-F1
Acc.
Multinomial
0.8491
0.8490
0.8528
0.8547
0.8550
Regres-
sion
3,
eigh
0.8974
0.9009
0.9010
5:
feature-set
e ect
(test
macro-F1)
eature
MNB
BNB
0.8992
0.6404
0.6441
0.8510
0.8539
Tw
quic
observ
ations
matc
um
ers.
First,
lexicon-only
eak
a
standalone
tation
but
addi
ng
them
top
BoW
bit
8992
!
9009
macro-F1).
Second,
McNemar
tests
sM
N
0000,
sB
0485)
supp
that
ot
just
umerically
etter
statistically
stronger
split.
Dot-as-Sen
tence-Boundary
(V2)
This
binary
classi er
texts
all
elds
except
label
inp
ut
prev
token
next
len
digit
before
erted
DictVectorizer
Regression
liblinear
solv
L2
L1
regularization,
01
100
1,
decision
threshold
scanning
0.10
0.89
0.01.
6:
vs
P
enalt
Threshold
F1
Precision
Recall
0.64
0.9993
0.9992
0.9980
0.9990
0.9985
0.59
0.9988
Both
arian
extremely
strong,
had
nal
selected
del.
high
scores
suggest
ailable
already
separate
oundary
non-b
cases
ery
ell
dataset.
UI
Note
implemen
ted
terfaces:
desktop
Tkin
ter
dash
oard
ui.py
lo
cal
Streamlit
app
localhost
).
er-task
execution
me
tr
ic
insp
ection,
while
Stream-
additionally
includes
small
teractiv
demos
analysis,
prediction,
dot-b
testing.