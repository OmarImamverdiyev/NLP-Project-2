\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\setlist[itemize]{noitemsep, topsep=2pt, leftmargin=*}
\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\setlength{\abovedisplayshortskip}{4pt}
\setlength{\belowdisplayshortskip}{4pt}

\title{NLP Project 2: Language Modelling and Text Classification}
\author{Omar Imamverdiyev \and Murad Valiyev}
\date{26 February 2026}

\begin{document}
\maketitle
\small

\section{Motivation}
This project applies classical NLP methods to Azerbaijani text.
The language is agglutinative and morphologically rich, so token sparsity is high and many higher-order n-grams are unseen even in relatively large corpora.
That makes Azerbaijani a good setting for:
\begin{itemize}
  \item testing how badly plain MLE fails without smoothing,
  \item comparing smoothing algorithms under realistic sparsity,
  \item evaluating lightweight sentiment classifiers with statistical tests, and
  \item learning sentence-boundary detection from weak supervision.
\end{itemize}

\section{Datasets and Preprocessing}
All metrics in this report come from the project run outputs in \texttt{test-results.txt} and the current repository code.

\subsection{News Corpus (Tasks 1, 2)}
For language modelling, we use \texttt{Corpora/News/corpus.txt}.
The full corpus contains 701{,}537 extracted sentences with the current sentence-splitting/tokenization pipeline.
For the reported run, we used a 120{,}000-sentence cap and split it into 80\%/10\%/10\% train/dev/test with seed 42:
\[
96{,}000 / 12{,}000 / 12{,}000.
\]
Sentence splitting uses regex \texttt{(?<=[.!?])\textbackslash s+}, and tokenization uses Unicode word boundaries \texttt{\textbackslash b\textbackslash w+\textbackslash b} on lowercased text.
Rare training tokens with count $<2$ are mapped to \texttt{<UNK>}.

\subsection{Sentiment Data (Task 3)}
Main Task 3 run uses \texttt{sentiment\_dataset/dataset\_v1.csv}:
\begin{itemize}
  \item 40{,}000 labeled texts,
  \item class distribution: 20{,}626 positive and 19{,}374 negative,
  \item positive ratio $=0.51565$ (near-balanced).
\end{itemize}
Split pipeline is stratified: first 80\% train-pool / 20\% test, then 20\% of train-pool as dev, giving:
\[
25{,}600 / 6{,}400 / 8{,}000.
\]

\subsection{Dot-Labeled Data (Task 4)}
Task 4 uses \texttt{dot\_labeled\_data.csv} with 200{,}000 labeled dot contexts:
\begin{itemize}
  \item label 0: 145{,}983
  \item label 1: 54{,}017
\end{itemize}
The split is stratified 70\% train and 30\% temp, then temp 50/50 into dev/test:
\[
140{,}000 / 30{,}000 / 30{,}000.
\]

\begin{table}[H]
\centering
\caption{Dataset summary used in this report}
\label{tab:dataset_summary}
\begin{tabular}{p{0.14\linewidth}p{0.31\linewidth}p{0.19\linewidth}p{0.24\linewidth}}
\toprule
Task & Source file & Total examples & Train / Dev / Test \\
\midrule
Task 1--2 & \texttt{Corpora/News/corpus.txt} & 701,537 sentences (120,000 used) & 96,000 / 12,000 / 12,000 \\
Task 3 & \texttt{sentiment\_dataset/dataset\_v1.csv} & 40,000 texts & 25,600 / 6,400 / 8,000 \\
Task 4 & \texttt{dot\_labeled\_data.csv} & 200,000 dot contexts & 140,000 / 30,000 / 30,000 \\
\bottomrule
\end{tabular}
\end{table}

\section{Task 1: N-gram Language Models (MLE)}
\subsection{Method}
We train unigram, bigram, and trigram models by maximum likelihood:
\[
P_{\text{uni}}(w)=\frac{C(w)}{N},\quad
P_{\text{bi}}(w_i\mid w_{i-1})=\frac{C(w_{i-1},w_i)}{C(w_{i-1})},
\]
\[
P_{\text{tri}}(w_i\mid w_{i-2},w_{i-1})=\frac{C(w_{i-2},w_{i-1},w_i)}{C(w_{i-2},w_{i-1})}.
\]
Perplexity on test tokens is:
\[
\mathrm{PPL}=\exp\!\left(
-\frac{1}{N}\sum_{i=1}^{N}\log P(w_i\mid \text{context}_i)
\right).
\]

\subsection{Results}
\begin{table}[H]
\centering
\caption{Task 1 test perplexity (MLE)}
\label{tab:task1}
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Vocabulary size (after \texttt{<UNK>}) & 58,418 \\
Unigram MLE perplexity & 3296.2567 \\
Bigram MLE perplexity & $\infty$ \\
Trigram MLE perplexity & $\infty$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}
The unigram model is finite because every test token is mapped into vocabulary (including \texttt{<UNK>}).
Bigram/trigram MLE are infinite because one unseen test n-gram yields probability 0, so $\log 0=-\infty$ and PPL diverges.

\section{Task 2: Smoothing for Bigram/Trigram LMs}
\subsection{Method}
We compare four methods:
\begin{itemize}
  \item Laplace (add-1):
  \[
  P_{\text{Lap}}(w\mid h)=\frac{C(h,w)+1}{C(h)+|V|}.
  \]
  \item Linear interpolation:
  \[
  P_{\text{interp}}=\lambda_1P_{\text{uni}}+\lambda_2P_{\text{bi}}+\lambda_3P_{\text{tri}},
  \quad \lambda_1+\lambda_2+\lambda_3=1.
  \]
  \item Absolute-discount backoff:
  \[
  P_{\text{bo}}(w\mid h)=\frac{\max(C(h,w)-d,0)}{C(h)}+\lambda(h)P_{\text{lower}}(w).
  \]
  \item Kneser--Ney: same discounting structure, but with continuation distribution:
  \[
  P_{\text{cont}}(w)=\frac{N_{1+}(\cdot,w)}{N_{1+}(\cdot,\cdot)}.
  \]
\end{itemize}
Dev tuning gave:
\[
(\lambda_1,\lambda_2)=(0.2,0.8)\ \text{for bigram interpolation},\quad
(\lambda_1,\lambda_2,\lambda_3)=(0.2,0.3,0.5)\ \text{for trigram interpolation}.
\]

\subsection{Results}
\begin{table}[H]
\centering
\caption{Task 2 test perplexity by smoothing method}
\label{tab:task2}
\begin{tabular}{lrr}
\toprule
Method & Bigram PPL & Trigram PPL \\
\midrule
Laplace & 3572.4528 & 12556.2180 \\
Interpolation & 167.9878 & 88.0896 \\
Backoff ($d=0.75$) & 158.2786 & 74.7584 \\
Kneser--Ney ($d=0.75$) & \textbf{150.0579} & \textbf{70.7616} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}
Kneser--Ney is best on both orders.
For trigrams, it reduces perplexity by approximately $99.44\%$ vs Laplace:
\[
\frac{12556.2180-70.7616}{12556.2180}\times 100 \approx 99.44\%.
\]
This matches standard findings: continuation-based lower-order probabilities are more informative than raw unigram frequency in sparse settings.

\section{Task 3: Sentiment Classification}
\subsection{Feature Space and Models}
We evaluate Multinomial NB, Bernoulli NB, and Logistic Regression on:
\begin{itemize}
  \item \texttt{bow}: unigram+bigram count vectors (\texttt{max\_features}=30{,}000, \texttt{min\_df}=2),
  \item \texttt{lexicon}: 6 handcrafted sentiment/negation cues,
  \item \texttt{bow\_lexicon}: concatenation.
\end{itemize}

For Multinomial NB:
\[
\hat{y}=\arg\max_y \left[\log P(y)+\sum_j x_j\log \theta_{y,j}\right].
\]
For logistic regression:
\[
P(y=1\mid x)=\sigma(w^\top x+b),\quad \sigma(z)=\frac{1}{1+e^{-z}},
\]
\[
\mathcal{L}(w,b)=
-\frac{1}{N}\sum_{i=1}^{N}\left[y_i\log p_i+(1-y_i)\log(1-p_i)\right]
+\lambda \|w\|_1.
\]
Hyperparameters are selected by dev macro-F1.

\subsection{Results}
\begin{table}[H]
\centering
\caption{Task 3 main model comparison}
\label{tab:task3_main}
\begin{tabular}{p{0.22\linewidth}p{0.17\linewidth}p{0.20\linewidth}rrr}
\toprule
Model & Best features & Best setting & Dev macro-F1 & Test macro-F1 & Test Acc. \\
\midrule
Multinomial NB & \texttt{bow} & $\alpha=0.05$ & 0.8491 & 0.8490 & 0.8490 \\
Bernoulli NB & \texttt{bow} & $\alpha=0.10$ & 0.8528 & 0.8547 & 0.8550 \\
Logistic Regression & \texttt{bow\_lexicon} & $C=0.3$, balanced & \textbf{0.8974} & \textbf{0.9009} & \textbf{0.9010} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Task 3 feature-set ablation (test macro-F1)}
\label{tab:task3_features}
\begin{tabular}{lrrr}
\toprule
Feature set & MNB & BNB & LR \\
\midrule
\texttt{bow} & 0.8490 & 0.8547 & 0.8992 \\
\texttt{lexicon} & 0.6404 & 0.6441 & 0.6441 \\
\texttt{bow\_lexicon} & \textbf{0.8510} & 0.8539 & \textbf{0.9009} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{McNemar exact test p-values}
\label{tab:mcnemar}
\begin{tabular}{lr}
\toprule
Comparison & p-value \\
\midrule
LR vs MNB & 0.0000 \\
LR vs BNB & 0.0000 \\
MNB vs BNB & 0.0485 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}
LR with \texttt{bow\_lexicon} is clearly strongest on this split.
The lexicon block alone is weak, but as a residual signal it improves LR from 0.8992 to 0.9009 macro-F1.
Significance testing confirms that LR's gain over both NB variants is not just noise.

\section{Task 4: Dot-as-Sentence-Boundary Classification (V2)}
\subsection{Method}
Each dot context is vectorized from \texttt{prev\_token}, \texttt{next\_token}, \texttt{prev\_len}, \texttt{next\_len}, and \texttt{is\_digit\_before} via \texttt{DictVectorizer}.
Binary logistic models are trained with L1 and L2 regularization:
\[
P(y=1\mid x)=\sigma(w^\top x+b).
\]
Decision threshold is tuned on dev:
\[
t^\star=\arg\max_{t\in\{0.10,0.11,\ldots,0.89\}} F_1^{\text{dev}}(t).
\]

\subsection{Results}
\begin{table}[H]
\centering
\caption{Task 4 L1 vs L2}
\label{tab:task4}
\begin{tabular}{lrrrrrrr}
\toprule
Penalty & Best $C$ & Threshold & Dev F1 & Test Acc. & Test Precision & Test Recall & Test F1 \\
\midrule
L2 & 10 & 0.64 & 0.9993 & 0.9992 & 0.9980 & 0.9990 & 0.9985 \\
L1 & 10 & 0.59 & 0.9993 & \textbf{0.9993} & \textbf{0.9985} & 0.9990 & \textbf{0.9988} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}
Both penalties are extremely strong; L1 is selected by best test F1.
Given how high these values are, this task appears close to linearly separable under the provided context features and labels.

\section{Overall Conclusions}
\begin{table}[H]
\centering
\caption{Best method by task}
\label{tab:summary}
\begin{tabular}{p{0.24\linewidth}p{0.34\linewidth}p{0.25\linewidth}}
\toprule
Task & Best method & Key result \\
\midrule
Task 1 (MLE LM) & Unigram (only finite MLE) & PPL $=3296.2567$ \\
Task 2 (Smoothing) & Kneser--Ney ($d=0.75$) & Trigram PPL $=70.7616$ \\
Task 3 (Sentiment) & Logistic Regression + \texttt{bow\_lexicon} & Macro-F1 $=0.9009$ \\
Task 4 (Boundary) & Logistic Regression (L1) & F1 $=0.9988$ \\
\bottomrule
\end{tabular}
\end{table}

The results reinforce standard NLP behavior on Azerbaijani data:
plain higher-order MLE is unusable without smoothing, Kneser--Ney is the most robust smoothing strategy in sparse regimes, and linear models with strong lexical features remain strong baselines for both sentiment and sentence-boundary detection.

\section{Team Contribution}
\begin{itemize}
  \item Omar Imamverdiyev: Unigram, Bigram, Trigram (with/without smoothing), Sentiment Analysis, Sentence Boundary.
  \item Murad Valiyev: Unigram, Bigram, Trigram (with/without smoothing), Sentence Boundary.
\end{itemize}

\newpage
\section*{References}
\begin{enumerate}
  \item Jurafsky, D. and Martin, J. H. (2023). \emph{Speech and Language Processing}, 3rd ed. draft.
  \item Chen, S. F. and Goodman, J. (1999). An empirical study of smoothing techniques for language modeling. \emph{Computer Speech and Language}, 13(4), 359--394.
  \item Kneser, R. and Ney, H. (1995). Improved backing-off for m-gram language modeling. In \emph{ICASSP}, 181--184.
  \item Manning, C. D. and Schutze, H. (1999). \emph{Foundations of Statistical Natural Language Processing}. MIT Press.
  \item Kiss, T. and Strunk, J. (2006). Unsupervised multilingual sentence boundary detection. \emph{Computational Linguistics}, 32(4), 485--525.
  \item McNemar, Q. (1947). Note on the sampling error of the difference between correlated proportions. \emph{Psychometrika}, 12(2), 153--157.
\end{enumerate}

\end{document}
